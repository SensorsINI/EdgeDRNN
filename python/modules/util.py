__author__ = "Chang Gao"
__copyright__ = "Copyright @ Chang Gao"
__credits__ = ["Chang Gao"]
__license__ = "Private"
__version__ = "0.0.1"
__maintainer__ = "Chang Gao"
__email__ = "gaochangw@outlook.com"
__status__ = "Prototype"

import os
import typing
import numpy as np
import torch
from torch import Tensor
from glob import glob
from torch.autograd.function import Function
import time
import math


def create_folder(folder_list):
    for folder in folder_list:
        try:
            os.makedirs(folder)
        except:
            pass


def timeSince(since):
    now = time.time()
    s = now - since
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)


def get_filepaths(dirpath: str, search_key: str):
    """
    :param dirpath: folder path to search
    :param search_key: key to search (e.g. search_key='*.wav')
    :return: A sorted list of file paths
    """
    filepaths = [y for x in os.walk(dirpath) for y in glob(os.path.join(x[0], search_key))]
    filepaths = sorted(filepaths)
    return filepaths


def get_filenames(filepaths: typing.List):
    return [os.path.basename(x) for x in filepaths]


def decode_ckpt_name(filename: str, num_metrics: int):
    metrics = filename.split('-')
    if len(metrics) == num_metrics + 1:
        metrics = metrics[1:]
    elif len(metrics) == num_metrics + 2:
        metrics = metrics[1:-1]
    else:
        raise ValueError('CKPT filename illegal!')
    metrics = '='.join(metrics).split('=')
    values = metrics[1::2]
    metrics = metrics[::2]
    dict_metrics = dict(zip(metrics, values))
    return dict_metrics


class GradPreserveRound(Function):
    @staticmethod
    def forward(ctx, input):
        output = torch.round(input)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        grad_input = grad_output
        if ctx.needs_input_grad[0]:
            grad_input = grad_output
        return grad_input


class GradPreserveFloor(Function):
    @staticmethod
    def forward(ctx, input):
        output = torch.floor(input)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        grad_input = grad_output
        if ctx.needs_input_grad[0]:
            grad_input = grad_output
        return grad_input


def quantize_tensor(x: Tensor, qi: int, qf: int, enable: int = 0, use_floor: bool = False) -> Tensor:
    """
    :param x: input tensor
    :param qi: number of integer bits before the decimal point
    :param qf: number of fraction bits after the decimal point
    :param enable: if 0, return x
    :param use_floor: Whether use floor() instead of round()
    :return: tensor quantized to fixed-point precision
    """
    if enable == 0:
        return x
    else:
        power = torch.tensor(float(2. ** qf), dtype=torch.float32)
        clip_val = torch.tensor(float(2. ** (qi + qf - 1) - 1), dtype=torch.float32)
        if use_floor:
            value = GradPreserveFloor.apply(x * power)
        else:
            value = GradPreserveRound.apply(x * power)  # Round Half to Even

        value = torch.max(value, -clip_val)
        value = torch.min(value, clip_val)
        # value = torch.clamp(value, -clip_val, clip_val - 1)  # saturation arithmetic
        value = torch.div(value, power)
        return value

def quantize_array(x, qi, qf, enable, unsigned=0):
    """
    :param x: input numpy array
    :param qi: number of integer bits before the decimal point
    :param qf: number of fraction bits after the decimal point
    :param enable: whether enable the function
    :param unsigned: whether unsigned input
    :return: array quantized to fixed-point precision
    """
    if enable == 0:
        return x
    power = np.asarray(2. ** qf).astype(np.float64)
    if unsigned == 0:
        clip_val = float(2. ** (qi + qf - 1))
        value = np.round(x * power)
        value = np.clip(value, -clip_val, clip_val - 1)  # saturation arithmetic
        result = value / power
    else:
        clip_val = float(2. ** (qi + qf))
        value_rnd = np.round(x * power)
        value = np.clip(value_rnd, 0, clip_val - 1)  # saturation arithmetic
        result = value / power
    return result

# def quantize_rnn(net, qi, qf, en):
#     for name, param in net.named_parameters():
#         if 'rnn' in name:
#             param.data = quantizeTensor(param.data, qi, qf, en)
#     return net

def cast_fxp_to_hex(x, data_type):
    x = np.asarray(x, dtype=data_type)
    return x

def cast_fp_to_fxp(x, qi, qf, use_floor=False):
    """
    Converts a floating point number into a fixed point number

    :param x: input numpy array
    :param qi: number of integer bits before the decimal point
    :param qf: number of fraction bits after the decimal point
    :return: tensor quantized to fixed-point precision
    """
    power = 2. ** qf
    clip_val = 2. ** (qi + qf - 1)
    value = x * power
    if use_floor:
        value = np.floor(value)  # rounding
    else:
        value = np.around(value)  # rounding
    value = np.clip(value, -clip_val, clip_val - 1).astype(np.int64)  # saturation arithmetic
    return value


def pruneTensor(x, alpha):
    """
    :param x: input tensor
    :param m: number of integer bits before the decimal point
    :param n: number of fraction bits after the decimal point
    :return: tensor quantized to fixed-point precision
    """

    n_neuron = x.size(0)
    n_input = x.size(1)
    prune_prob_mask = torch.exp(
        -alpha * torch.unsqueeze(torch.arange(0, n_neuron), dim=1).repeat(1, n_input).float()).cuda()
    prune_rand_mask = torch.rand(n_neuron, n_input).cuda()
    prune_mask = prune_rand_mask.masked_fill_(prune_rand_mask > prune_prob_mask, 1)
    prune_mask = prune_mask.masked_fill_(prune_rand_mask <= prune_prob_mask, 0)
    _, indices = torch.sort(torch.abs(x), 0)
    # print("indices shape", indices.size())
    # print("prune_mask shape", prune_mask.size())
    # print("x shape", x.size())
    for j in range(0, n_input):
        x[indices[:, j], j] *= prune_mask[:, j]

    return x


def targetedDropout(x, gamma, alpha, epoch):
    """
    :param x: input tensor
    :param m: number of integer bits before the decimal point
    :param n: number of fraction bits after the decimal point
    :return: tensor quantized to fixed-point precision
    """

    torch.manual_seed(epoch)
    torch.cuda.manual_seed_all(epoch)

    n_elements = x.numel()
    drop_part = round(n_elements * gamma)
    weight_vec = x.view(-1)
    weight_vec_abs = torch.abs(weight_vec)
    sorted, indices = torch.sort(weight_vec_abs)
    # print(sorted)
    drop_indices = indices[0:drop_part]
    drop_rand_mask = torch.rand(drop_indices.size(0)).cuda()
    drop_mask = torch.ones(drop_indices.size(0)).cuda()
    drop_mask = drop_mask.masked_fill_(drop_rand_mask <= alpha, 0)
    weight_vec[drop_indices] *= drop_mask
    weight = torch.reshape(weight_vec, (x.size(0), x.size(1)))

    return weight


def alignedTargetedDropout(x, gamma, alpha, num_pe, epoch):
    """
    :param x: input tensor
    :param m: number of integer bits before the decimal point
    :param n: number of fraction bits after the decimal point
    :return: tensor quantized to fixed-point precision
    """

    n_rows = x.shape[0]
    n_cols = x.shape[1]

    # Split and shuffle weight matrix
    for i in range(0, num_pe):
        for j in range(0, n_cols):
            targetedDropout(x[np.arange(i, n_rows, num_pe), j], gamma, alpha, epoch)
    return x


def look_ahead_seq(seq_in, t_width=16, padding=0, batch_first=0):
    # Convert input sequence to batch first shape (seq_len, n_batch, n_feature)
    seq = seq_in
    if batch_first:
        seq = seq_in.transpose(0, 1)

    seq_len = seq.size(0)
    n_batch = seq.size(1)
    n_feature = seq.size(2)
    # int(torch.ceil(float(seq_len)/float(t_width)))
    new_seq = []
    for i in range(0, seq_len):
        if i < seq_len - t_width:
            seq_block = seq[i:i + t_width, :, :]
        else:
            seq_block = seq[i:, :, :]
            seq_block_pad = torch.zeros([t_width - (seq_len - i), n_batch, n_feature], dtype=torch.float32).cuda()
            seq_block = torch.cat((seq_block, seq_block_pad), 0)
        new_seq.append(seq_block)
    new_seq = torch.stack(new_seq, 0)
    new_seq = new_seq.transpose(1, 2)
    new_seq = new_seq.transpose(0, 1)
    new_seq = new_seq.transpose(2, 3)
    return new_seq


def look_around_seq(seq_in, t_width=16, padding=0, batch_first=0):
    # Convert input sequence to batch first shape (seq_len, n_batch, n_feature)
    seq = seq_in
    if batch_first:
        seq = seq_in.transpose(0, 1)

    seq_len = seq.size(0)
    n_batch = seq.size(1)
    n_feature = seq.size(2)
    # int(torch.ceil(float(seq_len)/float(t_width)))
    new_seq = []
    for i in range(0, seq_len):
        if i >= seq_len - t_width:
            seq_block = seq[i - t_width:, :, :]
            seq_block_pad = torch.zeros([t_width - (seq_len - i) + 1, n_batch, n_feature], dtype=torch.float32).cuda()
            seq_block = torch.cat((seq_block, seq_block_pad), 0)
        elif i < t_width:
            seq_block = seq[0:i + 1 + t_width, :, :]
            seq_block_pad = torch.zeros([t_width - i, n_batch, n_feature], dtype=torch.float32).cuda()
            seq_block = torch.cat((seq_block, seq_block_pad), 0)
        else:
            seq_block = seq[i - t_width:i + 1 + t_width, :, :]
        # print(seq_block.size())
        new_seq.append(seq_block)
    new_seq = torch.stack(new_seq, 0)
    new_seq = new_seq.transpose(1, 2)
    new_seq = new_seq.transpose(0, 1)
    new_seq = new_seq.transpose(2, 3)
    return new_seq


def get_temporal_sparsity(list_layer, seq_len, threshold):
    # Evaluate Sparsity
    num_zeros = 0
    num_elems = 0
    # print(seq_len.size())
    # Iterate through nnlayers
    for layer in list_layer:
        all_delta_vec = layer.transpose(0, 1)
        all_delta_vec = torch.abs(all_delta_vec)  # Take absolute values of all delta vector elements
        for i, delta_vec in enumerate(all_delta_vec):
            seq = delta_vec[:seq_len[i], :]
            zero_mask = seq < threshold
            num_zeros += torch.sum(zero_mask)
            num_elems += torch.numel(zero_mask)
    sparsity = float(num_zeros) / float(num_elems)
    return sparsity


def gen_xilinx_sdk_clib(dir_path, file_name, dict_params):
    """
    Generate C Library for NN parameters
    :param dir_path: dir path to export the file
    :param filename: name of the C Library
    :param dict_params:    Dictionary from which to save numpy arrays of NN parameters in fp
    :param qdict:    Dictionary from which to save properties of corresponding variables in pdict in tuples
        Elements of each tuple in qdict in order:
        - quantize
        - qi:        number of integer bits before the decimal point in fxp
        - qf:        number of integer bits after the decimal point in fxp
        - var_class: 0 - Constant | 1 - General Tensors | 2 - NN Parameter |
        - print_row_size
    :return:         mat_param_fxp - parameter in fixed-point numbers
    """

    # Create file
    try:
        f_h = open(dir_path + '/' + file_name + '.h', 'w')
    except IOError as err:
        print("I/O error({0}): {1}".format(err.errno, err.strerror))
    try:
        f_c = open(dir_path + '/' + file_name + '.c', 'w')
    except IOError as err:
        print("I/O error({0}): {1}".format(err.errno, err.strerror))

    def get_var_type(quantize, bit_width):
        if quantize:
            # Get variable type
            if 0 < bit_width <= 8:
                var_type = 'signed char'
                num_aligned_bytes = 8
            elif 8 < bit_width <= 16:
                var_type = 'short'
                num_aligned_bytes = 8
            elif 16 < bit_width <= 32:
                var_type = 'long'
                num_aligned_bytes = 8
            else:
                raise ValueError('Bit width of parameter must not be larger than 32...')
        else:
            var_type = 'float'
            num_aligned_bytes = 16
        return var_type, num_aligned_bytes

    # Write macro definitions
    macro_name = file_name.upper() + '_H'
    f_h.write('#ifndef ' + macro_name + '\n' + '#define ' + macro_name + '\n')
    f_c.write('#include "' + file_name + '.h"\n')

    # Iterate over params in dict
    pdict_out = {}
    for key, value in dict_params.items():
        # Get quantization precision of the variable
        temp_value, cast, qi, qf, var_class, print_row_size = value
        temp_value = np.asarray(temp_value)
        bit_width = qi + qf

        # Convert fp to fxp
        if cast:
            pdict_out[key] = cast_fp_to_fxp(temp_value, qi, qf)
        else:
            pdict_out[key] = temp_value

        # Get variable type
        var_type, num_aligned_bytes = get_var_type(cast, bit_width)

        if var_class == 'const':  # Constant
            f_h.write('#define ' + key.upper() + ' %d\n' % pdict_out[key])
        elif var_class == 'tensor':  # General Tensor
            if pdict_out[key].ndim == 1:
                pdict_out[key] = np.reshape(pdict_out[key].shape[0], -1)

            # Get param dimension
            size = pdict_out[key].size
            dim0 = pdict_out[key].shape[0]
            dim1 = pdict_out[key].shape[1]

            # Write variable declaration
            f_h.write('/*\n'
                      ' * Var Type: General Matrix \n'
                      ' * Var Name: %s' % key + '\n'
                                                ' * Bit Width: %d' % bit_width + '\n'
                                                                                 ' * Dimension: (%d, %d)' % (
                          dim0, dim1) + '\n'
                                        ' */\n'
                      )
            f_h.write('#define ' + key.upper() + '_NUM_ROWS %d\n' % dim0)
            f_h.write('#define ' + key.upper() + '_NUM_COLS %d\n' % dim1)
            f_h.write('#define ' + key.upper() + '_MAT_SIZE %d\n' % size)
            f_h.write('extern const %s %s[' % (var_type, key) + key.upper() + '_MAT_SIZE];\n')
            f_c.write('const %s %s[' % (var_type, key) + key.upper() + '_MAT_SIZE] = {\n')

            # Write parameters column-wisely
            for i in range(0, dim1):
                for j in range(0, dim0):
                    if var_type == 'float':
                        f_c.write('%.20f' % pdict_out[key][j][i])
                    else:
                        f_c.write('%d' % pdict_out[key][j][i])
                    if not (i >= dim1 - 1 and j >= dim0 - 1):
                        f_c.write(',')
                    if (i * dim0 + j) % print_row_size == print_row_size - 1:
                        f_c.write('\n')
            f_c.write('};\n\n')

        elif var_class == 'param':  # NN Parameter
            if pdict_out[key].ndim == 1:
                pdict_out[key] = np.reshape(pdict_out[key].shape[0], -1)

            # Get param dimension
            size = pdict_out[key].size
            dim0 = pdict_out[key].shape[0]
            dim1 = pdict_out[key].shape[1]

            # Write variable declaration
            f_h.write('/*\n'
                      ' * Var Type: NN Parameter Matrix \n'
                      ' * Var Name: %s' % key + '\n'
                                                ' * Bit Width: %d' % bit_width + '\n'
                                                                                 ' * Dimension: (%d, %d)' % (
                          dim0, dim1) + '\n'
                                        ' */\n'
                      )

            f_h.write('#define ' + key.upper() + '_SIZE %d\n' % size)
            f_h.write('extern const %s %s[' % (
                var_type, key) + key.upper() + '_SIZE] __attribute__ ((aligned (%d)));\n' % num_aligned_bytes)
            f_c.write('const %s %s[' % (var_type, key) + key.upper() + '_SIZE] = {\n')

            # Write parameters column-wisely
            for i in range(0, dim1):
                for j in range(0, dim0):
                    write_val = pdict_out[key][j][i]
                    if isinstance(write_val, float):
                        f_c.write('%f' % write_val)
                    else:
                        f_c.write('%d' % write_val)
                    if not (i == dim1 - 1 and j == dim0 - 1):
                        f_c.write(',')
                    if (i * dim0 + j) % print_row_size == print_row_size - 1:
                        f_c.write('\n')
            f_c.write('};\n\n')

    f_h.write('#endif')
    f_c.close()
    f_h.close()

    return pdict_out

# def gen_sim_file(dir_path, dict_params, qi, qf):
#     """
#     Generate Modelsim Simulation Files for NN parameters
#     :param dir_path: dir path to export files
#     :param dict_params:    Dictionary from which to save numpy arrays of NN parameters in fp
#     :param qi:       number of integer bits before the decimal point in fxp
#     :param qf:       number of integer bits after the decimal point in fxp
#     :return:         mat_param_fxp - parameter in fixed-point numbers
#     """
#
#     # Get unsigned variable type
#     bit_width = qi + qf
#     if 0 < bit_width <= 8:
#         var_type = 'uint8'
#     elif 8 < bit_width <= 16:
#         var_type = 'uint16'
#     elif 16 < bit_width <= 32:
#         var_type = 'uint32'
#     else:
#         raise ValueError('Bit width of parameter must not be larger than 32...')
#
#     # Iterate over params in dict
#     pdict_out = {}
#     for key, value in dict_params.items():
#         # Convert fp to fxp
#         pdict_out[key] = cast_fp_to_fxp(value, qi, qf)
#
#         # Get param dimension
#         dim0 = value.shape[0]
#         dim1 = value.shape[1]
#         # Create file
#         try:
#             f = open(dir_path + '/' + key + '.txt', 'w')
#         except IOError as err:
#             print("I/O error({0}): {1}".format(err.errno, err.strerror))
#
#         # Write parameters column-wisely
#         param_hex = cast_fxp_to_hex(pdict_out[key], var_type)
#         for i in range(0, dim1):
#             for j in range(0, dim0):
#                 if var_type == 'uint8':
#                     f.write('%2.2X\n' % param_hex[j][i])
#                 elif var_type == 'uint16':
#                     f.write('%4.4X\n' % param_hex[j][i])
#                 elif var_type == 'uint32':
#                     f.write('%8.8X\n' % param_hex[j][i])
#                 else:
#                     f.write('%X\n' % param_hex[j][i])
#         f.close()
#
#     return pdict_out


def gen_sim_file(dir_path, dict_params, qi, qf, row_blocks=1):
    """
    Generate Modelsim Simulation Files for NN parameters

    :param dict_params:    Dictionary from which to save numpy arrays of NN parameters in fp
    :param qi:       number of integer bits before the decimal point in fxp
    :param qf:       number of integer bits after the decimal point in fxp
    :return:         mat_param_fxp - parameter in fixed-point numbers
    """

    # Get unsigned variable type
    bit_width = qi + qf
    if 0 < bit_width <= 8:
        var_type = 'uint8'
    elif 8 < bit_width <= 16:
        var_type = 'uint16'
    elif 16 < bit_width <= 32:
        var_type = 'uint32'
    else:
        raise ValueError('Bit width of parameter must not be larger than 32...')

    # Iterate over params in dict
    pdict_out = {}
    for key, value in dict_params.items():
        # Convert fp to fxp
        pdict_out[key] = cast_fp_to_fxp(value, qi, qf, use_floor=False)

        # Get param dimension
        n_rows = value.shape[0]
        n_cols = value.shape[1]

        # Create file
        try:
            f = open(os.path.join(dir_path, key + '.txt'), 'w')
        except:
            raise RuntimeError("Failed to create file.")

        def write_hex(f, x, var_type):
            x_hex = cast_fxp_to_hex(x, var_type)
            if var_type == 'uint8':
                f.write('%2.2X' % x_hex)
            elif var_type == 'uint16':
                f.write('%4.4X' % x_hex)
            elif var_type == 'uint32':
                f.write('%8.8X' % x_hex)
            else:
                f.write('%X' % x_hex)

        # Write parameters column-wisely
        param_print = pdict_out[key]
        for i_col in range(0, n_cols):
            block_buffer = np.zeros(row_blocks, dtype=np.int32)
            i_block = 0
            for i_row in range(0, n_rows):
                block_buffer[i_block] = param_print[i_row][i_col]
                i_block = i_block + 1
                if i_block == row_blocks:
                    for block_elem in reversed(block_buffer):
                        write_hex(f, block_elem, var_type)
                    f.write(' //')
                    for block_elem in reversed(block_buffer):
                        f.write(' %d,' % block_elem)
                    i_block = 0
                    f.write('\n')
        f.close()

    return pdict_out